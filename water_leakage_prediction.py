# -*- coding: utf-8 -*-
"""water_leakage_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BGX_Kx8ujqE-0F1r4b0SemaVcSuyQggO
"""

from google.colab import files
files.upload()  # buradan kaggle.json dosyanı seç

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ziya07/water-leak-dataset
!unzip water-leak-dataset.zip -d water_leak_data

import pandas as pd

df = pd.read_csv("water_leak_data/water_leak_detection_1000_rows.csv")
print(df.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import joblib

# CSV dosyasını oku
df = pd.read_csv("water_leak_data/water_leak_detection_1000_rows.csv")
print(df.head())

# Features / Target
X = df.drop(columns=["Leak Status"])
y = df["Leak Status"]

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

# Sayısal sütunları seç
numeric_cols = X_train.select_dtypes(include=['int64','float64']).columns

# Scaler
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])

models = {
    "RandomForest": RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, class_weight=class_weights_dict),
    "XGBoost": xgb.XGBClassifier(n_estimators=100, max_depth=5, use_label_encoder=False, eval_metric="logloss", scale_pos_weight=class_weights_dict[1]/class_weights_dict[0]),
    "LogisticRegression": LogisticRegression(max_iter=500, random_state=42, class_weight=class_weights_dict)
}

# Timestamp -> hour, day, month
X_train_enc = X_train.copy()
X_test_enc = X_test.copy()

if 'Timestamp' in X_train_enc.columns:
    X_train_enc['Timestamp'] = pd.to_datetime(X_train_enc['Timestamp'])
    X_test_enc['Timestamp'] = pd.to_datetime(X_test_enc['Timestamp'])

    X_train_enc['Hour'] = X_train_enc['Timestamp'].dt.hour
    X_train_enc['Day'] = X_train_enc['Timestamp'].dt.day
    X_train_enc['Month'] = X_train_enc['Timestamp'].dt.month

    X_test_enc['Hour'] = X_test_enc['Timestamp'].dt.hour
    X_test_enc['Day'] = X_test_enc['Timestamp'].dt.day
    X_test_enc['Month'] = X_test_enc['Timestamp'].dt.month

    # Orijinal Timestamp sütunu düşür
    X_train_enc = X_train_enc.drop(columns=['Timestamp'])
    X_test_enc = X_test_enc.drop(columns=['Timestamp'])

# Sensor_ID varsa one-hot
if 'Sensor_ID' in X_train_enc.columns:
    X_train_enc = pd.get_dummies(X_train_enc, columns=['Sensor_ID'], drop_first=True)
    X_test_enc = pd.get_dummies(X_test_enc, columns=['Sensor_ID'], drop_first=True)

# Logistic Regression için scale
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_enc)
X_test_scaled = scaler.transform(X_test_enc)

results = {}

for name, model in models.items():
    if name == "LogisticRegression":
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train_enc, y_train)
        y_pred = model.predict(X_test_enc)

    acc = accuracy_score(y_test, y_pred)
    print(f"\n{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))
    results[name] = acc

from sklearn.utils import class_weight
import numpy as np

# Calculate class weights
class_weights = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights_dict = dict(enumerate(class_weights))

print("Class Weights:", class_weights_dict)

best_model_name = max(results, key=results.get)
print(f"\nEn iyi model: {best_model_name} Accuracy = {results[best_model_name]:.4f}")

best_model = models[best_model_name]

# Modeli kaydet
joblib.dump(best_model, f"{best_model_name}_water_leak_model.pkl")
print(f"{best_model_name}_water_leak_model.pkl kaydedildi!")

# Eğer Logistic Regression en iyi ise scaler’ı da kaydet
if best_model_name == "LogisticRegression":
    joblib.dump(scaler, "scaler.pkl")
    print("Scaler.pkl kaydedildi!")



"""# Task
Adjust XGBoost model hyperparameters to mitigate overfitting, specifically tuning `max_depth`, `min_child_weight`, `subsample`, and `colsample_bytree`, and then apply cross-validation to assess the model's generalizability.

## XGBoost Hiperparametre Ayarlaması

### Subtask:
XGBoost modelinin aşırı uyumunu azaltmak için `max_depth`, `min_child_weight`, `subsample`, `colsample_bytree` gibi hiperparametreleri ayarlayacağız. Bu, modelin veri üzerindeki karmaşıklığını kontrol etmeye yardımcı olacaktır.

**Reasoning**:
To systematically search for the best hyperparameters for the XGBoost model, I need to use GridSearchCV. This involves defining a parameter grid and fitting the GridSearchCV object to the training data.
"""

from sklearn.model_selection import GridSearchCV
import xgboost as xgb

# Define the parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# Initialize XGBoost classifier with class weights
# class_weights_dict is available from previous steps: {0: 0.5115, 1: 22.222}
xgb_model = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric="logloss",
    scale_pos_weight=class_weights_dict[1]/class_weights_dict[0],
    random_state=42
)

# Create GridSearchCV object
# Using cv=3 for quicker demonstration, can be increased to 5 for more robust results
# Using 'f1' as scoring metric because the dataset is imbalanced (as suggested in the instructions)
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='f1',
    cv=3,
    verbose=1,
    n_jobs=-1
)

# Fit GridSearchCV to the training data
grid_search.fit(X_train_enc, y_train)

# Print the best parameters and best score
print("Best parameters found: ", grid_search.best_params_)
print("Best F1 score found: ", grid_search.best_score_)

"""**Reasoning**:
The previous step successfully identified the best hyperparameters for the XGBoost model using GridSearchCV. Now, to complete the subtask, I will initialize a new XGBoost model with these optimal parameters and evaluate its performance on the test set.


"""

import xgboost as xgb
from sklearn.metrics import classification_report, accuracy_score

# Get the best parameters from GridSearchCV
best_xgb_params = grid_search.best_params_

# Initialize a new XGBoost classifier with the best parameters and class weights
best_xgb_model = xgb.XGBClassifier(
    **best_xgb_params,
    use_label_encoder=False,
    eval_metric="logloss",
    scale_pos_weight=class_weights_dict[1]/class_weights_dict[0],
    random_state=42
)

# Train the model with the best parameters on the full training data
best_xgb_model.fit(X_train_enc, y_train)

# Make predictions on the test set
y_pred_tuned_xgb = best_xgb_model.predict(X_test_enc)

# Evaluate the tuned model
acc_tuned_xgb = accuracy_score(y_test, y_pred_tuned_xgb)
print(f"\nTuned XGBoost Accuracy: {acc_tuned_xgb:.4f}")
print(classification_report(y_test, y_pred_tuned_xgb))

# Optionally, update the results dictionary if needed, or compare with previous XGBoost result
results["XGBoost_Tuned"] = acc_tuned_xgb
print(f"Updated results dictionary with Tuned XGBoost accuracy: {results['XGBoost_Tuned']:.4f}")

"""**Reasoning**:
The previous step successfully tuned the XGBoost model and evaluated its performance. To complete the model development process for the tuned model, it's essential to save this best-performing model to disk for future use, similar to how the original best model was saved.


"""

import joblib

# Save the best-tuned XGBoost model
joblib.dump(best_xgb_model, "XGBoost_tuned_water_leak_model.pkl")
print("XGBoost_tuned_water_leak_model.pkl saved successfully!")

"""## Final Task

### Subtask:
Analyze the tuned XGBoost model's performance, summarizing the impact of hyperparameter tuning and cross-validation on mitigating overfitting and its effectiveness, especially for the minority class.

## Summary:

### Q&A
*   **Impact of hyperparameter tuning and cross-validation on mitigating overfitting**: Hyperparameter tuning with `GridSearchCV` and 3-fold cross-validation, using parameters like `max_depth`, `min_child_weight`, `subsample`, and `colsample_bytree`, aimed to control model complexity and improve generalization. The process yielded an F1-score of approximately 0.65 for the best parameters during tuning, suggesting a balanced performance during model selection.
*   **Effectiveness, especially for the minority class**: The tuned model achieved a perfect recall of 1.00 for the minority class (class 1), meaning it successfully identified all actual positive cases. However, its precision for the minority class was low at 0.25, indicating a high number of false positive predictions for this class. This trade-off is reflected in an F1-score of 0.40 for the minority class.

### Data Analysis Key Findings
*   The optimal hyperparameters identified through `GridSearchCV` were `colsample_bytree: 0.6`, `max_depth: 5`, `min_child_weight: 5`, and `subsample: 0.6`, yielding a best F1 score of approximately 0.6496 during cross-validation.
*   The tuned XGBoost model achieved an overall accuracy of 0.9850 on the test set.
*   For the majority class (class 0), the model demonstrated strong performance with a precision of 1.00, recall of 0.98, and an F1-score of 0.99.
*   For the minority class (class 1), the model achieved a recall of 1.00 but had a low precision of 0.25, resulting in an F1-score of 0.40.
*   The best-tuned XGBoost model was successfully saved as `XGBoost_tuned_water_leak_model.pkl`.

### Insights or Next Steps
*   The significant disparity between recall (1.00) and precision (0.25) for the minority class indicates that while the model is very good at detecting all potential leakages, it also generates many false alarms. Further steps should focus on improving precision for the minority class, possibly by adjusting the classification threshold or exploring cost-sensitive learning techniques.
*   To further validate the model's robustness and address the precision issue for the minority class, it would be beneficial to perform a detailed error analysis, specifically examining the characteristics of the false positive predictions.
"""

import joblib
import pandas as pd

# Modeli yükle
model = joblib.load("/content/XGBoost_tuned_water_leak_model.pkl")

# Test datasından ön işlenmiş 5 örnek al
sample_for_prediction = X_test_enc.iloc[:5]
pred = model.predict(sample_for_prediction)
print(pred)

from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import pandas as pd

app = FastAPI()
model = joblib.load("XGBoost_tuned_water_leak_model.pkl")

class SensorData(BaseModel):
    Flow_Rate: float
    Pressure_bar: float
    Temperature: float
    Burst_Status: int
    # Diğer numeric feature'lar

@app.post("/predict")
def predict(data: SensorData):
    df = pd.DataFrame([data.dict()])
    pred = model.predict(df)[0]
    return {"Leak_Status": int(pred)}



# Uygulamayı çalıştırmak için terminalde şu komutu kullanın:
# uvicorn water_leakage_prediction:app --reload